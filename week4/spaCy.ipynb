{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d868dcf76e7299",
   "metadata": {},
   "source": [
    "### Simple One-liners\n",
    "\n",
    "<b>Python one-liners</b> are short programs that perform powerful operations, doing a lot within a single line of code.\n",
    "\n",
    "One-liners are very common in text processing, which is why we’re introducing a few examples here to help you better understand the code we are going to show today. Run the code cell below, can you explain what is happening in it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "daf303936c8d9091",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T21:24:25.078084Z",
     "start_time": "2025-11-19T21:24:25.075108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "# Simple for loop\n",
    "result = [x * 2 for x in range(5)]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c9c4a5624eb8b",
   "metadata": {},
   "source": [
    "A list can also be created as a one-liner with conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e185c0b4ae49e538",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T21:22:21.355650Z",
     "start_time": "2025-11-19T21:22:21.352981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'd', 'f']\n"
     ]
    }
   ],
   "source": [
    "letters = list(\"abCdEfG\")\n",
    "\n",
    "lower = []\n",
    "for letter in letters:\n",
    "    if letter.islower():\n",
    "        lower.append(letter)\n",
    "\n",
    "# The code above does the same thing as this one-liner:\n",
    "lower = [letter for letter in letters if letter.islower()]\n",
    "print(lower)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7f2327a620eb4",
   "metadata": {},
   "source": [
    "One can even add transformations on top of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e5f2e86faf3ff003",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T21:40:43.037650Z",
     "start_time": "2025-11-19T21:40:43.034683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'from', 'python']\n"
     ]
    }
   ],
   "source": [
    "words = \"Hello World from Python !\".split(\" \")\n",
    "lower = [w.lower() for w in words if w.isalpha()]\n",
    "print(lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8562fd73a2004f0f",
   "metadata": {},
   "source": [
    "In general such one-liners follow the following template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf080b434143e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[expression for item in iterable if condition]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a161d17c0b709817",
   "metadata": {},
   "source": [
    "#### Exercise 1:  One-liner\n",
    "1. From a list of integers, write a one-liner that keeps only the even numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1c3aeeede5e9dc3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T21:36:05.857070Z",
     "start_time": "2025-11-19T21:36:05.850857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 10, 14]\n"
     ]
    }
   ],
   "source": [
    "numbers = [3, 4, 7, 10, 11, 14]\n",
    "# even_number ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab9593d042addd6",
   "metadata": {},
   "source": [
    "2. Take a list of words, convert each to uppercase, and concatenate them into one string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "6f0f37f2d9259097",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T09:11:27.499114Z",
     "start_time": "2025-11-20T09:11:27.496622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHAT IS THE MENSA MENU TODAY\n"
     ]
    }
   ],
   "source": [
    "words = [\"how\",\"was\",\"the\",\"mensa\",\"today\"]\n",
    "# sentence ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d72f3b55ef0b338",
   "metadata": {},
   "source": [
    "---\n",
    "### spaCy\n",
    "#### Installation\n",
    "Before you install spaCy and its dependencies, make sure that your `pip`, `setuptools` and `wheel` are up to date by clicking the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f181563602febf17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T13:33:23.882070Z",
     "start_time": "2025-11-19T13:33:22.725239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/cqx931/projects/leuphana/2025_WS/AsWeMaySpeak/.venv/lib/python3.12/site-packages (23.2.1)\r\n",
      "Requirement already satisfied: setuptools in /Users/cqx931/projects/leuphana/2025_WS/AsWeMaySpeak/.venv/lib/python3.12/site-packages (80.9.0)\r\n",
      "Requirement already satisfied: wheel in /Users/cqx931/projects/leuphana/2025_WS/AsWeMaySpeak/.venv/lib/python3.12/site-packages (0.45.1)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install setuptools wheel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92acd5c0065440ec",
   "metadata": {},
   "source": [
    "Then install spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd28a027e02347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2871c644dd8927",
   "metadata": {},
   "source": [
    "Or you can do the steps above directly in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d01f03037d368",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U pip setuptools wheel\n",
    "pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b4416517dc9f15",
   "metadata": {},
   "source": [
    "After installing spaCy, you will also need to download a language model. Use the following cell to download a basic English language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2aa4fe7cadafcd88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T13:38:50.793648Z",
     "start_time": "2025-11-19T13:38:46.195343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m33.5/33.5 MB\u001B[0m \u001B[31m13.2 MB/s\u001B[0m  \u001B[33m0:00:02\u001B[0mm0:00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: en-core-web-md\r\n",
      "Successfully installed en-core-web-md-3.8.0\r\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_md')\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd5555402ffc22",
   "metadata": {},
   "source": [
    "Now let's create a new spaCy object using `spacy.load()`. What you put as the parameter here shall match with the model you downloaded earlier. If you want to try another model, you shall change the name as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "681d92f7877e0e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T13:46:45.927797Z",
     "start_time": "2025-11-19T13:46:44.305284Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25584daaf50bec38",
   "metadata": {},
   "source": [
    "After that we can use this spaCy object to parse a short piece of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e353895aa6c32aa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T13:49:22.715701Z",
     "start_time": "2025-11-19T13:49:22.675516Z"
    }
   },
   "outputs": [],
   "source": [
    "# We are taking here one paragraph from The Picture of Dorian Gray as an example\n",
    "text = \"Dorian Gray hurried along the quay through the drizzling rain. His meeting with Adrian Singleton had strangely moved him, and he wondered if the ruin of that young life was really to be laid at his door, as Basil Hallward had said to him with such infamy of insult. He bit his lip, and for a few seconds his eyes grew sad. Yet, after all, what did it matter to him? One's days were too brief to take the burden of another's errors on one's shoulders. Each man lived his own life, and paid his own price for living it. The only pity was one had to pay so often for a single fault. One had to pay over and over again, indeed. In her dealings with man Destiny never closed her accounts.\"\n",
    "\n",
    "doc = nlp(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb45b469ac9f6e6",
   "metadata": {},
   "source": [
    "---\n",
    "#### Tokenization and Word Counter\n",
    "\n",
    "By simply passing our text to spaCy, it is going to tokenize the text and give us the following basic information about the text:\n",
    "1. All of the sentences (doc.sents)\n",
    "1. All of the words (doc)\n",
    "1. All of the \"named entities,\": names of places, people, #brands, etc. (doc.ents)\n",
    "1. All none phrases or \"noun_chunks\": nouns in the text plus surrounding matter like adjectives and articles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e61749e3e18ca0d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T21:02:40.656430Z",
     "start_time": "2025-11-19T21:02:40.653326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are in total 9 sentences in the text.\n",
      "Sample sentences:\n",
      "In her dealings with man Destiny never closed her accounts.\n",
      "Dorian Gray hurried along the quay through the drizzling rain.\n",
      "One had to pay over and over again, indeed.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 1. sentences\n",
    "sentences = list(doc.sents)\n",
    "print(\"There are in total\", len(sentences), \"sentences in the text.\")\n",
    "\n",
    "print(\"Sample sentences:\")\n",
    "for item in random.sample(sentences, 3):\n",
    "    print(item.text.strip().replace(\"\\n\", \" \"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2688c54e0dcf651a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a686f685e70127e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T21:11:11.810222Z",
     "start_time": "2025-11-19T21:11:11.802929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dorian, Gray, hurried, along, the, quay, through, the, drizzling, rain, His, meeting, with, Adrian, Singleton, had, strangely, moved, him, and, he, wondered, if, the, ruin, of, that, young, life, was, really, to, be, laid, at, his, door, as, Basil, Hallward, had, said, to, him, with, such, infamy, of, insult, He, bit, his, lip, and, for, a, few, seconds, his, eyes, grew, sad, Yet, after, all, what, did, it, matter, to, him, One, days, were, too, brief, to, take, the, burden, of, another, errors, on, one, shoulders, Each, man, lived, his, own, life, and, paid, his, own, price, for, living, it, The, only, pity, was, one, had, to, pay, so, often, for, a, single, fault, One, had, to, pay, over, and, over, again, indeed, In, her, dealings, with, man, Destiny, never, closed, her, accounts]\n"
     ]
    }
   ],
   "source": [
    "# 2. words\n",
    "# tokens = [token for token in doc]\n",
    "words = [token for token in doc if token.is_alpha]\n",
    "#print(tokens)\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27e868957e2c47d",
   "metadata": {},
   "source": [
    "The list of words that we are composing here is actually a list of spaCy [Token](https://spacy.io/api/token) objects. To compose a list of words, we are using `.is_alpha` attribute from the Token class, which returns true if the token consist of alphabetic characters. In the code cell above, compare `tokens` and `words`, what's the difference?\n",
    "\n",
    "<b>Entities</b> are important in NLP because they usually contain information about the “who/what/where,” making them the most information-dense parts of a text. Identifying them helps us quickly understand the main topic. It also automatically group multi-word concepts for you. The process of extracting entities from text is called <b>Named Entity Recognition (NER)</b>. The NER label scheme varies by language and depends heavily on the training data available. You can find all available entity types for `en_core_web_sm` [here](https://spacy.io/models/en#en_core_web_sm-labels).\n",
    "\n",
    "`noun_chunks` are also very useful in this regard. They are commonly used for text summarization and keyword extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "daccea73dc0ba8fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T14:37:29.666421Z",
     "start_time": "2025-11-19T14:37:29.660256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dorian Gray PERSON\n",
      "Adrian Singleton PERSON\n",
      "Basil Hallward PERSON\n",
      "a few seconds TIME\n",
      "One CARDINAL\n",
      "days DATE\n",
      "Noun phrases: ['Dorian Gray', 'the quay', 'the drizzling rain', 'His meeting', 'Adrian Singleton', 'him', 'he', 'the ruin', 'that young life', 'his door', 'Basil Hallward', 'him', 'such infamy', 'insult', 'He', 'his lip', 'a few seconds', 'his eyes', 'what', 'it', 'him', \"One's days\", 'the burden', \"another's errors\", \"one's shoulders\", 'Each man', 'his own life', 'his own price', 'it', 'The only pity', 'a single fault', 'her dealings', 'man Destiny', 'her accounts']\n"
     ]
    }
   ],
   "source": [
    "# 3. Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "\n",
    "# 4. Noun phrases\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376893d8a20250b3",
   "metadata": {},
   "source": [
    "We can also visualise entities from a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "1cdb7af3c20a681f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T08:46:59.269019Z",
     "start_time": "2025-11-20T08:46:59.253324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">&quot;As We May Think&quot; is an essay written by \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Vannevar Bush\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1945\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". He is an \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    American\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " engineer.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_demo = nlp('\"As We May Think\" is an essay written by Vannevar Bush in 1945. He is an American engineer.')\n",
    "displacy.render(doc_demo, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b35caed6636313",
   "metadata": {},
   "source": [
    "This is more useful if we load the whole text and try to take a look in it. This can take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "48c58785fe35bb34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:41:11.902561Z",
     "start_time": "2025-11-19T15:40:55.989582Z"
    }
   },
   "outputs": [],
   "source": [
    "text = open(\"../week3/pg26740.txt\").read()\n",
    "full_doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f851179e17884e7",
   "metadata": {},
   "source": [
    "And let's make a word counter and print the 10 most common words from the text. What do you expect the result to be like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d7ced407682afccd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:45:04.780011Z",
     "start_time": "2025-11-19T15:45:04.646892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 3558), ('of', 2286), ('and', 2195), ('to', 2153), ('I', 1694), ('a', 1629), ('that', 1302), ('in', 1233), ('you', 1146), ('was', 1066)]\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_words =  [token for token in full_doc if token.is_alpha]\n",
    "word_count = Counter([w.text for w in all_words])\n",
    "\n",
    "print(word_count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8206797188d81f7",
   "metadata": {},
   "source": [
    "We see that it's not so useful to analyse these words as they don't represent the content of the text. These words are called <b>stop words</b> and there is a list for such words in spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "af069600f9c1ae15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:57:18.262016Z",
     "start_time": "2025-11-19T15:57:18.258622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'once', 'others', 'here', 'third', 'elsewhere', 'throughout', 'call', 'his', 'done', 'nevertheless', 'i', 'onto', 'being', '’ll', 'somehow', '’re', 'not', 'as', 'very', \"n't\", 'less', 'why', 'nobody', 'well', 'whenever', 'is', 'whereupon', 'at', 'however', 'latterly', 'few', 'then', 'should', 'get', 'due', 'last', 'sixty', 'just', '‘d', 'if', 'four', 'keep', 'n’t', 'forty', 'before', 'again', 'over', 'both', 'afterwards', 'than', 'amongst', 'against', 'while', 'used', 'my', 'and', 'everything', 'who', 'same', 'since', 'wherever', 'did', 'full', 'himself', 'are', 'make', 'one', 'thereupon', 'anything', 'besides', 'everyone', 'another', 'some', 'serious', 'any', 'was', 'itself', 'noone', 'whence', 'can', '‘ll', 'were', 'around', 'next', 'you', 'perhaps', 'seemed', 'hundred', 'into', 'fifty', 'whereby', 'our', 'top', 'many', 'mine', 'also', 'anyhow', 'herein', 'except', 'bottom', 'would', 'because', 'hence', 'too', 'part', 'herself', 'though', 'about', 'anywhere', 'else', 'those', 'had', 'thereby', 'how', 'no', 'that', 'therein', 'whither', 'sometimes', 'up', 'various', 'thence', 'after', 'amount', 'everywhere', 'towards', 'where', 'thus', 'doing', 'formerly', 'themselves', 'off', 'anyone', 'whether', 'former', 'meanwhile', 'wherein', 'hereby', 'something', 'might', 'must', 'hereupon', 'thru', 'each', 'seem', 'made', 'per', 'go', 'ourselves', 'first', 'beyond', 'seems', 'which', 'six', 'hereafter', 'ten', 'nine', 'along', 'empty', 'your', 'other', \"'s\", 'what', 'further', 'for', 'none', 'does', 'cannot', 'quite', 'am', 'somewhere', 'still', 'beforehand', 'during', 'yet', 'least', 'thereafter', 'whom', 'nothing', 'myself', 'among', 'do', 'alone', '’s', 'have', 'eight', 'ever', 'say', 'name', 'such', 'we', 'her', '‘s', 'above', 'becomes', 'ours', 'whereafter', 'she', 'someone', \"'ve\", 'really', \"'m\", 'him', 'please', 'even', 'of', 'move', 'been', 'hers', '‘re', 'without', 'every', 'it', 'whoever', 'regarding', 'a', \"'ll\", 'already', 'be', 'whereas', 'becoming', 'either', 'several', 'whole', 'within', '’m', 'namely', 'often', 'side', 'see', 'under', 'latter', 'become', 'to', 'so', 'put', 'two', 'own', 'its', 'but', 'nowhere', 'below', 'take', 'never', 'moreover', 'always', 'whose', 'these', \"'re\", 'enough', 'much', 'their', 'the', 'indeed', 'by', 'between', 'they', 'may', 'me', 'an', 'them', 'has', 'anyway', 'using', 'eleven', 'toward', 'with', 'otherwise', 'three', 'back', 'behind', 'nor', 'via', 'from', 'twenty', 'almost', 'give', 'could', \"'d\", 'all', 'unless', 'us', 'together', 'became', 'there', 'will', 'more', 'in', 'he', 'beside', '‘ve', 'or', 'show', 'upon', 'seeming', 'n‘t', 'whatever', 'through', 'front', '’d', 'on', 'only', 'therefore', 'yours', 'five', '’ve', 'when', 'neither', 'until', 'yourselves', 'this', 'mostly', 'fifteen', 'twelve', 'down', 'now', 'sometime', 'most', 'ca', 'rather', 'out', 'across', 'although', 're', 'yourself', '‘m'}\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceef0482168f6a2",
   "metadata": {},
   "source": [
    "We can then go ahead to remove these words from our word counter to see some more meaningful statistics from our word counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7655015e230837a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T07:45:03.076003Z",
     "start_time": "2025-11-20T07:45:03.030141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83533 33956\n",
      "[('Dorian', 417), ('said', 262), ('Lord', 245), ('Henry', 236), ('like', 221), ('life', 217), ('Gray', 204), ('man', 179), ('know', 175), ('Harry', 175), ('Basil', 158), ('things', 126), ('think', 126), ('thing', 121), ('eyes', 109), ('good', 107), ('come', 107), ('face', 106), ('want', 105), ('time', 103)]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# create a list of words without stop words, also considering different cases\n",
    "all_words_without_sw = [word for word in all_words if word.text.lower() not in STOP_WORDS]\n",
    "print(len(all_words), len(all_words_without_sw))\n",
    "word_count = Counter([w.text for w in all_words_without_sw])\n",
    "print(word_count.most_common(20))\n",
    "most_common_adj = [adj for word in all_words_without_sw if word.pos_ == \"ADJECTIVE\" ]\n",
    "print(most_common_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf390f0a08993e",
   "metadata": {},
   "source": [
    "It is also possible to check if a token is part of stop words by using the `.is_stop` attribute from `Token` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "621399e1c288889f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T20:48:58.536154Z",
     "start_time": "2025-11-19T20:48:58.533264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'> Dorian False\n",
      "<class 'spacy.tokens.token.Token'> Gray False\n",
      "<class 'spacy.tokens.token.Token'> hurried False\n",
      "<class 'spacy.tokens.token.Token'> along True\n",
      "<class 'spacy.tokens.token.Token'> the True\n"
     ]
    }
   ],
   "source": [
    "sample = words[:5]\n",
    "for word in sample:\n",
    "    print(type(word), word.text, word.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daac7be38ecab40f",
   "metadata": {},
   "source": [
    "Depending on the text you want to analyze, you can also customize the default stop-word list by adding or removing words as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c3c664d90a91726",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T20:58:20.804281Z",
     "start_time": "2025-11-19T20:58:20.801866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "STOP_WORDS.remove('again')\n",
    "print(\"again\" in STOP_WORDS)\n",
    "STOP_WORDS.add('again')\n",
    "print(\"again\" in STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a16b36f42c73e6",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "- Load a text file of your choice\n",
    "- Create a word frequency counter\n",
    "- Improve the results by applying preprocessing steps (lowercasing, removing stop words, etc.)\n",
    "- Use spaCy to extract additional information from the text, such as named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a61aba1021bebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15672627479e2261",
   "metadata": {},
   "source": [
    "---\n",
    "#### POS(Parts of speech) Tagging\n",
    "After tokenization, spaCy can parse and tag a given Doc. POS tags provide information about a word in its context. spaCy provides such tagging in two systemsg: `.pos_` uses [universal POS tags](https://universaldependencies.org/u/pos/), while `.tag_` follows [Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) system. The Penn Treebank POS tags are more detailed than the universal ones as it has different tags for verb in different tenses or noun is plural/singular forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "55642a521039a359",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T08:34:19.431791Z",
     "start_time": "2025-11-20T08:34:19.420753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The / DET / DT\n",
      "quick / ADJ / JJ\n",
      "brown / ADJ / JJ\n",
      "fox / NOUN / NN\n",
      "jumps / VERB / VBZ\n",
      "over / ADP / IN\n",
      "one / NUM / CD\n",
      "of / ADP / IN\n",
      "the / DET / DT\n",
      "lazy / ADJ / JJ\n",
      "dogs / NOUN / NNS\n",
      ". / PUNCT / .\n",
      "DT JJ JJ NN VBZ IN CD IN DT JJ NNS .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_demo = nlp(\"The quick brown fox jumps over one of the lazy dogs.\")\n",
    "for token in doc_demo:\n",
    "    print(token.text, \"/\", token.pos_, \"/\", token.tag_)\n",
    "\n",
    "# Printing the sentence in tag\n",
    "demo_in_tag = \" \".join([token.tag_ for token in doc_demo])\n",
    "print(demo_in_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2b7445feca1b77",
   "metadata": {},
   "source": [
    "Most of the tags and labels look pretty abstract, and they vary between languages. You can use `spacy.explain()` to show you a short description of what a tag means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "1e646731492c35b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T07:57:46.474149Z",
     "start_time": "2025-11-20T07:57:46.471144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'verb, 3rd person singular present'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"VBZ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba43483e1bad80cf",
   "metadata": {},
   "source": [
    "We can use `pos_` or `tag_` to filter a particular type of words that we want from a text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "928c6de0e9f8a735",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T07:59:26.875033Z",
     "start_time": "2025-11-20T07:59:26.871614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouns: ['quay', 'drizzling', 'rain', 'meeting', 'ruin', 'life', 'door', 'infamy', 'insult', 'lip', 'seconds', 'eyes', 'days', 'burden', 'errors', 'shoulders', 'man', 'life', 'price', 'pity', 'fault', 'dealings', 'man', 'accounts']\n",
      "Verbs in past tense: ['hurried', 'had', 'wondered', 'was', 'had', 'bit', 'grew', 'did', 'were', 'lived', 'paid', 'was', 'had', 'had', 'closed']\n"
     ]
    }
   ],
   "source": [
    "print(\"Nouns:\", [token.text for token in doc if token.pos_ == \"NOUN\"])\n",
    "print(\"Verbs in past tense:\", [token.text for token in doc if token.tag_ == \"VBD\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a007f096a3ce9",
   "metadata": {},
   "source": [
    "---\n",
    "#### Lemmatization and Inflection\n",
    "When we are compling a list of words from a text corpus, sometimes it makes sense to save the words in their most basic form. For that we will need the lemma of a word, or the process of lemmatization.\n",
    "\n",
    "A word's \"lemma\" is its most \"basic\" form, the form without any morphology applied to it. In the example above we have the word \"moved\", the past tense of \"move\", or \"seconds\", the plural form of \"second\".\n",
    "\n",
    "For example, we can get all the verbs, nouns, adjectives and adverbs without morphology from the example text with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6360485309f624df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T07:47:20.998117Z",
     "start_time": "2025-11-20T07:47:20.995110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbs: ['hurry', 'move', 'wonder', 'lay', 'say', 'bite', 'grow', 'matter', 'take', 'live', 'pay', 'live', 'have', 'pay', 'have', 'pay', 'close']\n",
      "Nouns: ['quay', 'drizzling', 'rain', 'meeting', 'ruin', 'life', 'door', 'infamy', 'insult', 'lip', 'second', 'eye', 'day', 'burden', 'error', 'shoulder', 'man', 'life', 'price', 'pity', 'fault', 'dealing', 'man', 'account']\n",
      "Adjectives: ['young', 'such', 'few', 'sad', 'brief', 'own', 'own', 'only', 'single']\n",
      "Adverbs: ['strangely', 'really', 'yet', 'after', 'all', 'too', 'so', 'often', 'over', 'again', 'indeed', 'never']\n"
     ]
    }
   ],
   "source": [
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "print(\"Nouns:\", [token.lemma_ for token in doc if token.pos_ == \"NOUN\"])\n",
    "print(\"Adjectives:\", [token.lemma_ for token in doc if token.pos_ == \"ADJ\"])\n",
    "print(\"Adverbs:\", [token.lemma_ for token in doc if token.pos_ == \"ADV\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b099d6e0c410d",
   "metadata": {},
   "source": [
    "The opposite process of lemmatization is called <b>Inflection</b>, when we want to change the form of a verb/noun according to its current context (like number, tense, case...).\n",
    "\n",
    "If we want to make sure that the words in a generated sentence is grammatically correct no matter what words got randomly chosen from a list, we could use a python library called `LemmInflect`. The system acts as a standalone module or as an extension to spaCy and it only works with English words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T16:06:08.088522Z",
     "start_time": "2025-11-19T16:06:07.566903Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: LemmInflect in /Users/cqx931/projects/leuphana/2025_WS/AsWeMaySpeak/.venv/lib/python3.12/site-packages (0.2.3)\r\n",
      "Requirement already satisfied: numpy in /Users/cqx931/projects/leuphana/2025_WS/AsWeMaySpeak/.venv/lib/python3.12/site-packages (from LemmInflect) (2.3.5)\r\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install LemmInflect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8818a39e7729ba5c",
   "metadata": {},
   "source": [
    "Here is a demo of some commonly used transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9782edd7f04b2745",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:18:19.970925Z",
     "start_time": "2025-11-19T15:18:19.966271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('was', 'were')\n",
      "('being',)\n",
      "('been',)\n",
      "('am', 'are')\n",
      "('is',)\n",
      "('teeth',)\n",
      "('media', 'mediums')\n",
      "('better',)\n",
      "('best',)\n"
     ]
    }
   ],
   "source": [
    "from lemminflect import getInflection\n",
    "\n",
    "# Verb Demo\n",
    "print(getInflection('be', tag='VBD'))\n",
    "print(getInflection('be', tag='VBG'))\n",
    "print(getInflection('be', tag='VBN'))\n",
    "print(getInflection('be', tag='VBP'))\n",
    "print(getInflection('be', tag='VBZ'))\n",
    "\n",
    "# Noun Demo\n",
    "print(getInflection('tooth', tag='NNS'))\n",
    "print(getInflection('medium', tag='NNS'))\n",
    "\n",
    "# Noun Demo\n",
    "print(getInflection('good', tag='JJR'))\n",
    "print(getInflection('good', tag='JJS'))\n",
    "\n",
    "# pos_type = 'A'\n",
    "# * JJ      Adjective\n",
    "# * JJR     Adjective, comparative\n",
    "# * JJS     Adjective, superlative\n",
    "# * RB      Adverb\n",
    "# * RBR     Adverb, comparative\n",
    "# * RBS     Adverb, superlative\n",
    "#\n",
    "# pos_type = 'N'\n",
    "# * NN      Noun, singular or mass\n",
    "# * NNS     Noun, plural\n",
    "#\n",
    "# pos_type = 'V'\n",
    "# * VB      Verb, base form\n",
    "# * VBD     Verb, past tense\n",
    "# * VBG     Verb, gerund or present participle\n",
    "# * VBN     Verb, past participle\n",
    "# * VBP     Verb, non-3rd person singular present\n",
    "# * VBZ     Verb, 3rd person singular present\n",
    "# * MD      Modal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209e5a4baa416137",
   "metadata": {},
   "source": [
    "---\n",
    "### Sentence Level Analysis\n",
    "\n",
    "To understand how words are connected within a sentence, spaCy uses [Dependency grammar](https://en.wikipedia.org/wiki/Dependency_grammar) as its framework to process the text. In this framework, each word depends on a “head” word their dependency relation can be categorized into pre-defined types. It helps spaCy understand sentence structure so it can figure out things like subjects, objects, and how different parts of the sentence relate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "4057094e13fe029",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T08:47:37.961158Z",
     "start_time": "2025-11-20T08:47:37.950619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word: The\n",
      "Tag: DT\n",
      "Head: fox\n",
      "Dependency relation: det\n",
      "Subtree: The\n",
      "\n",
      "Word: quick\n",
      "Tag: JJ\n",
      "Head: fox\n",
      "Dependency relation: amod\n",
      "Subtree: quick\n",
      "\n",
      "Word: brown\n",
      "Tag: JJ\n",
      "Head: fox\n",
      "Dependency relation: amod\n",
      "Subtree: brown\n",
      "\n",
      "Word: fox\n",
      "Tag: NN\n",
      "Head: jumps\n",
      "Dependency relation: nsubj\n",
      "Subtree: The quick brown fox\n",
      "\n",
      "Word: jumps\n",
      "Tag: VBZ\n",
      "Head: jumps\n",
      "Dependency relation: ROOT\n",
      "Subtree: The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "Word: over\n",
      "Tag: IN\n",
      "Head: jumps\n",
      "Dependency relation: prep\n",
      "Subtree: over the lazy dog\n",
      "\n",
      "Word: the\n",
      "Tag: DT\n",
      "Head: dog\n",
      "Dependency relation: det\n",
      "Subtree: the\n",
      "\n",
      "Word: lazy\n",
      "Tag: JJ\n",
      "Head: dog\n",
      "Dependency relation: amod\n",
      "Subtree: lazy\n",
      "\n",
      "Word: dog\n",
      "Tag: NN\n",
      "Head: over\n",
      "Dependency relation: pobj\n",
      "Subtree: the lazy dog\n",
      "\n",
      "Word: .\n",
      "Tag: .\n",
      "Head: jumps\n",
      "Dependency relation: punct\n",
      "Subtree: .\n"
     ]
    }
   ],
   "source": [
    "doc_demo = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
    "\n",
    "def flatten_subtree(st):\n",
    "    return ''.join([w.text_with_ws for w in st]).strip()\n",
    "\n",
    "for token in doc_demo:\n",
    "    print()\n",
    "    print(\"Word:\", token.text)\n",
    "    print(\"Tag:\", token.tag_)\n",
    "    print(\"Head:\", token.head.text)\n",
    "    print(\"Dependency relation:\", token.dep_)\n",
    "    print(\"Subtree:\", flatten_subtree(token.subtree))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b31c342dc9f6ba",
   "metadata": {},
   "source": [
    "Spacy has its own visualizer and we can use it to help us understand what's happening here in an easier way. In the visualisation we can see that \"The quick brown fox\" are grouped together under the head \"fox\" because it is the center of this sequence. Same happens with \"the lazy dog\". It is in this way how spaCy picks up the noun chunks."
   ]
  },
  {
   "cell_type": "code",
   "id": "45f975ff6a1c57ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T12:55:14.584405Z",
     "start_time": "2025-11-20T12:55:14.560653Z"
    }
   },
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc_demo, style=\"dep\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"a0ea9b8a6df04a2787323d194fdb1924-0\" class=\"displacy\" width=\"1625\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">quick</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">brown</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">fox</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">jumps</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">over</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">lazy</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">dog.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a0ea9b8a6df04a2787323d194fdb1924-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 575.0,2.0 575.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a0ea9b8a6df04a2787323d194fdb1924-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a0ea9b8a6df04a2787323d194fdb1924-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a0ea9b8a6df04a2787323d194fdb1924-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a0ea9b8a6df04a2787323d194fdb1924-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a0ea9b8a6df04a2787323d194fdb1924-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a0ea9b8a6df04a2787323d194fdb1924-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a0ea9b8a6df04a2787323d194fdb1924-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a0ea9b8a6df04a2787323d194fdb1924-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a0ea9b8a6df04a2787323d194fdb1924-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a0ea9b8a6df04a2787323d194fdb1924-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,89.5 1445.0,89.5 1445.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a0ea9b8a6df04a2787323d194fdb1924-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a0ea9b8a6df04a2787323d194fdb1924-0-6\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a0ea9b8a6df04a2787323d194fdb1924-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,266.5 L1287,254.5 1303,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a0ea9b8a6df04a2787323d194fdb1924-0-7\" stroke-width=\"2px\" d=\"M945,264.5 C945,2.0 1450.0,2.0 1450.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a0ea9b8a6df04a2787323d194fdb1924-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1450.0,266.5 L1458.0,254.5 1442.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 257
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "3a21b41fc3f2770b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T08:32:36.518182Z",
     "start_time": "2025-11-20T08:32:36.512214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[The quick brown fox, the lazy dog]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[chunk for chunk in doc_demo.noun_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81069441365cc6f1",
   "metadata": {},
   "source": [
    "Let's see some concrete examples how this can be useful.  For example, we can get a whole phrase by its head using `Token.subtree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "94d2fc3208dd6a31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T08:56:20.044881Z",
     "start_time": "2025-11-20T08:56:19.980180Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of London',\n",
       " 'of it',\n",
       " 'about music',\n",
       " 'At last',\n",
       " 'of being stared at which',\n",
       " 'As for what I said to you to-night',\n",
       " 'to supper']"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_phrases = [] # prepositional phrase\n",
    "for token in full_doc:\n",
    "    if token.dep_ == 'prep':\n",
    "        prep_phrases.append(flatten_subtree(token.subtree).replace(\"\\n\", \" \")) # replace line break with space\n",
    "\n",
    "random.sample(prep_phrases, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "8db7fb9e0316b484",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T09:20:33.694110Z",
     "start_time": "2025-11-20T09:20:33.636035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['if you wish it',\n",
       " 'I wish to see it',\n",
       " '\"  \"I wish she were ill',\n",
       " 'if you wish to know the exact time',\n",
       " 'I wish you would tell me your secret.',\n",
       " '\"I wish you had seen him.',\n",
       " 'though I wish you chaps would not squabble over the picture',\n",
       " 'I wish I had now.',\n",
       " 'I wish that I had ever had such an experience.',\n",
       " '\"  \"Ah, Alan,\" murmured Dorian, with a sigh, \"I wish you had a thousandth part of the pity for me that I have for you.\"']"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compose list of phrases with exact text matching\n",
    "\n",
    "def phrases_with_word(text):\n",
    "    return [flatten_subtree(token.subtree).replace(\"\\n\", \" \") for token in full_doc if token.text == text]\n",
    "\n",
    "# random.sample(phrases_with('with'),10)\n",
    "random.sample(phrases_with_word('wish'),10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b052d1fa89a04a",
   "metadata": {},
   "source": [
    "We are covering only the basic usages of spaCy today. For more tutorials or documentations, you can check out [the official website](https://spacy.io/usage/spacy-101). We will learn more about it in the later sessions of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284cba17f8708cce",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "- Extract groups of words from your text. For example: lists of nouns, verbs, adjectives...\n",
    "- Try to do that also with longer phrases. Look closely at your text, what phrases might be interesting to extract?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c769607c4ab6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93383ace03ce6948",
   "metadata": {},
   "source": [
    "---\n",
    "### Assignment 2\n",
    "Option 1\n",
    "1. Find a text corpus that interests you\n",
    "2. Use spaCy to process and harvest groups of words/phrases\n",
    "3. Then use `tracery`to build a text generator based on the harvested material\n",
    "\n",
    "Option 2\n",
    "1. Find a text corpus that interests you\n",
    "2. Use spaCy to tag the text\n",
    "3. Use `Markovify`to generate new sentences based on tags\n",
    "4. Replace the tag with other words (either from same text or other resources)\n",
    "\n",
    "Or feel free to come up with your own approach, as long as you use spaCy to help you create a text generator.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
